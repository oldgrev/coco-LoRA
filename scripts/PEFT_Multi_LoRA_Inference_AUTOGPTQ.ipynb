{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from auto_gptq.eval_tasks import TextSummarizationTask\n",
    "from transformers import AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36460935",
   "metadata": {
    "id": "36460935"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "base_model_dir = \"/home/grev/models/decapoda-research_llama-13b-quant\"\n",
    "use_fast_tokenizer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85af699",
   "metadata": {
    "id": "d85af699"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir, use_fast=use_fast_tokenizer)\n",
    "if not tokenizer.pad_token_id:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    base_model_dir,\n",
    "    use_triton=True,\n",
    "    warmup_triton=False,\n",
    "    trainable=True,\n",
    "    inject_fused_attention=True,\n",
    "    inject_fused_mlp=False\n",
    ")\n",
    "model.warmup_triton()\n",
    "device = model.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f515ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0f515ed",
    "outputId": "312488a5-f4f8-48a4-8c63-7b4a59e80418"
   },
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, get_gptq_peft_model\n",
    "from auto_gptq.utils.data_utils import make_data_block, collate_data\n",
    "from auto_gptq.utils.peft_utils import GPTQLoraConfig\n",
    "from peft import TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_gptq_peft_model(model, auto_find_all_linears=True, train_mode=False, model_id=\"/home/grev/code/gqtl/gptqlora/outputoscar1/checkpoint-100/adapter_model\",adapter_name=\"oscar1\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df02b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0c121",
   "metadata": {
    "id": "67a0c121"
   },
   "outputs": [],
   "source": [
    "model.load_adapter(\"/home/grev/code/gqtl/gptqlora/outputoscar2/checkpoint-100/adapter_model\", adapter_name=\"oscar2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_adapter(\"oscar1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_adapter(\"oscar2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.disable_adapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b655fca",
   "metadata": {
    "id": "4b655fca"
   },
   "outputs": [],
   "source": [
    "model.active_peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ebd572",
   "metadata": {
    "id": "e9ebd572"
   },
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138805b3",
   "metadata": {
    "id": "138805b3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Input:\n",
    "{input}\n",
    "### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\"\"\"\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=256,\n",
    "    **kwargs,\n",
    "):\n",
    "    torch.manual_seed(42)\n",
    "    prompt = generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        no_repeat_ngram_size=3,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output.split(\"### Response:\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4991822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(instruction, input=None):\n",
    "    return instruction\n",
    "\n",
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=256,\n",
    "    **kwargs,\n",
    "):\n",
    "    torch.manual_seed(42)\n",
    "    prompt = generate_prompt(instruction, input)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "        no_repeat_ngram_size=3,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33650851",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33650851",
    "outputId": "aae24052-0f09-4812-88c3-6fb53dec656c"
   },
   "outputs": [],
   "source": [
    "instruction = \"Complete the below:\\nThe winner of the 2023 Oscar ACTOR IN A LEADING ROLE was\"\n",
    "\n",
    "print(evaluate(instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da89abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"The winner of the 2023 Oscar CINEMATOGRAPHY was\"\n",
    "\n",
    "print(evaluate(instruction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e4e9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b8e4e9a",
    "outputId": "84226223-e018-4feb-e189-969c344fd940"
   },
   "outputs": [],
   "source": [
    "with model.disable_adapter():\n",
    "    instruction = \"Complete the below:\\nThe winner of the 2023 Oscar ACTOR IN A LEADING ROLE was\"\n",
    "\n",
    "    print(evaluate(instruction))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
